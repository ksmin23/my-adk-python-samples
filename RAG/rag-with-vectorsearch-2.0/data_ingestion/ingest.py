#!/usr/bin/env python3
# -*- encoding: utf-8 -*-
# vim: tabstop=2 shiftwidth=2 softtabstop=2 expandtab

import argparse
import os
import uuid
import logging

from dotenv import load_dotenv
from google.cloud import vectorsearch_v1beta
from langchain_community.document_loaders import DirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from tqdm.auto import tqdm

load_dotenv()
logging.basicConfig(
  level=logging.INFO,
  format='%(asctime)s - %(levelname)s - %(message)s'
)


def ingest_documents(
  project_id: str,
  location: str,
  collection_name: str,
  source_dir: str,
  batch_size: int = 250
):
  """
  Ingest documents into Vertex AI Vector Search 2.0.

  This function uses Vector Search 2.0's auto-embedding feature.
  When data objects are created without vectors, the service automatically
  generates embeddings using the model configured in the collection's
  vector_schema.

  Args:
      project_id: GCP Project ID
      location: GCP Location
      collection_name: Vector Search Collection name
      source_dir: Directory containing source documents
      batch_size: Number of documents per batch (max 250 for auto-embedding)
  """
  # Load and split documents
  logging.info(f"Loading documents from {source_dir}...")
  docs = []
  for glob_pattern in ["**/*.md", "**/*.txt"]:
    loader = DirectoryLoader(source_dir, glob=glob_pattern, show_progress=True)
    docs.extend(loader.load())

  if not docs:
    logging.warning("No documents found.")
    return

  text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=100
  )
  chunks = text_splitter.split_documents(docs)
  logging.info(f"Split into {len(chunks)} chunks.")

  # Initialize Vector Search 2.0 Data Object Service Client
  data_object_client = vectorsearch_v1beta.DataObjectServiceClient()

  parent = f"projects/{project_id}/locations/{location}/collections/{collection_name}"

  # Process chunks in batches
  # Batch size should not exceed embedding model's max texts per request
  # (250 for text-embedding-005 and gemini-embedding-001)
  total_batches = (len(chunks) + batch_size - 1) // batch_size

  for batch_start in tqdm(
    range(0, len(chunks), batch_size),
    desc="Importing documents",
    unit="batch"
  ):
    batch_end = min(batch_start + batch_size, len(chunks))
    batch = chunks[batch_start:batch_end]

    # Build batch request following Vector Search 2.0 SDK pattern
    # Each item in requests should have data_object_id and data_object
    batch_request = []
    for chunk in batch:
      # Generate unique ID based on content
      doc_id = str(uuid.uuid5(uuid.NAMESPACE_DNS, chunk.page_content))

      batch_request.append({
        "data_object_id": doc_id,
        "data_object": {
          # Data fields (must match collection's data_schema)
          "data": {
            "content": chunk.page_content,
          },
          # Empty vectors - will be auto-generated by Vector Search 2.0
          # based on the vertex_embedding_config in the collection's vector_schema
          "vectors": {},
        },
      })

    try:
      request = vectorsearch_v1beta.BatchCreateDataObjectsRequest(
        parent=parent,
        requests=batch_request,  # Note: 'requests' not 'data_objects'
      )
      data_object_client.batch_create_data_objects(request)
    except Exception as e:
      if "already exists" not in str(e).lower():
        logging.error(
          f"Batch {batch_start // batch_size + 1}/{total_batches} error: "
          f"{str(e)[:100]}"
        )

  logging.info(f"âœ… Import complete! Processed {len(chunks)} chunks.")


def main():
  parser = argparse.ArgumentParser(
    description="Ingest documents into Vector Search 2.0"
  )
  parser.add_argument(
    "--project_id",
    default=os.getenv("GOOGLE_CLOUD_PROJECT"),
    help="GCP Project ID (default: from GOOGLE_CLOUD_PROJECT env var)"
  )
  parser.add_argument(
    "--location",
    default=os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1"),
    help="GCP Location (default: from GOOGLE_CLOUD_LOCATION env var or 'us-central1')"
  )
  parser.add_argument(
    "--collection_name",
    default=os.getenv("VECTOR_SEARCH_COLLECTION_NAME"),
    help="Collection Name (default: from VECTOR_SEARCH_COLLECTION_NAME env var)"
  )
  parser.add_argument(
    "--source_dir",
    default="source_documents",
    help="Source documents directory"
  )
  parser.add_argument(
    "--batch_size",
    type=int,
    default=250,
    help="Batch size (max 250 for auto-embedding)"
  )

  args = parser.parse_args()

  if not args.project_id or not args.collection_name:
    raise ValueError("Project ID and Collection Name are required.")

  ingest_documents(
    args.project_id,
    args.location,
    args.collection_name,
    args.source_dir,
    args.batch_size
  )


if __name__ == "__main__":
  main()
